LLM-Compare-Hub Project File Structure and Use Cases
====================================================

Core Application Files
---------------------

gradio_full_llm_eval.py (1.0B)
- Use Case: Main Gradio web interface
- Function: Orchestrates the entire application, provides user-friendly web UI
- Features: Prompt input, response display, evaluation results, analysis generation
- Status: Modular - delegates all logic to other files

response_generator.py (8.6KB)
- Use Case: LLM response generation and comparison
- Function: Generates responses from GPT-4, Claude 3, and Gemini 1.5
- Features: Side-by-side response comparison, batch processing, optional evaluation
- Status: Standalone tool + used by Gradio app

round_robin_evaluator.py (9.3KB)
- Use Case: Comprehensive model evaluation system
- Function: Each model evaluates all other models (GPT-4 evaluates Claude/Gemini, etc.)
- Features: Multi-metric scoring, CSV export, detailed reasoning
- Status: Core evaluation engine used by all other components

llm_prompt_eval_analysis.py (12KB)
- Use Case: Data analysis and visualization
- Function: Analyzes evaluation results, generates charts and reports
- Features: Statistical analysis, correlation matrices, performance comparisons
- Status: Standalone analysis tool + used by Gradio app

llm_response_logger.py (6.3KB)
- Use Case: Quick testing and logging tool
- Function: Rapid testing of all models with single or batch prompts
- Features: Quick evaluation, CSV export, batch processing
- Status: Standalone testing tool + used by Gradio app

Supporting Modules
-----------------

realtime_detector.py (923B)
- Use Case: Real-time query detection
- Function: Determines if a prompt needs current information
- Features: Uses GPT-3.5-turbo to classify real-time vs. general queries
- Status: Utility module used by response generation

search_fallback.py (1.6KB)
- Use Case: Google search integration
- Function: Fetches current information for real-time queries
- Features: Google Custom Search API integration, result formatting
- Status: Utility module used by response generation

Configuration & Documentation
----------------------------

requirements.txt (232B)
- Use Case: Python dependencies
- Function: Lists all required packages and versions
- Features: Gradio, OpenAI, Anthropic, Google AI, pandas, matplotlib, etc.
- Status: Essential for project setup

.env (not shown - should exist)
- Use Case: API key configuration
- Function: Stores all API keys securely
- Features: OpenAI, Claude, Gemini, Google Search API keys
- Status: Essential for functionality

.gitignore (661B)
- Use Case: Git version control
- Function: Excludes sensitive files from version control
- Features: API keys, results, cache files, etc.
- Status: Essential for security

README.md (4.1KB)
- Use Case: Project documentation
- Function: Setup instructions, usage guide, feature descriptions
- Features: Installation, configuration, usage examples
- Status: Essential for users and collaborators

Testing & Development
--------------------

test_standalone_tools.py (4.1KB)
- Use Case: Testing and demonstration
- Function: Shows how to use all standalone tools
- Features: Quick start guide, sample prompts, tool explanations
- Status: Development/testing tool

__pycache__/ (directory)
- Use Case: Python cache
- Function: Stores compiled Python bytecode
- Features: Improves import performance
- Status: Auto-generated, can be deleted

Generated Files (when running the app)
-------------------------------------

results/ (directory - created when needed)
- Use Case: Evaluation results storage
- Function: Stores CSV files with evaluation data
- Features: Timestamped files, comprehensive evaluation data
- Status: Auto-generated during evaluation

analysis_results/ (directory - created when needed)
- Use Case: Analysis output storage
- Function: Stores charts, reports, and visualizations
- Features: Performance charts, correlation matrices, analysis reports
- Status: Auto-generated during analysis

Project Summary
==============

Your project has a clean, modular architecture with:
- 4 core functional modules (response generation, evaluation, analysis, logging)
- 2 utility modules (real-time detection, search integration)
- 1 main interface (Gradio web app)
- Complete configuration (requirements, environment, documentation)
- Testing tools for development and demonstration

All files serve specific purposes and work together to provide a comprehensive LLM comparison and evaluation system.

Key Features:
- Multi-model response generation (GPT-4, Claude 3, Gemini 1.5)
- Comprehensive round-robin evaluation system
- Real-time query detection and search integration
- Advanced data analysis and visualization
- Batch processing capabilities
- Clean, production-ready code without emojis
- Modular architecture for maintainability
- Complete web interface via Gradio
- Standalone tools for automation and testing

Usage:
1. Set up API keys in .env file
2. Install dependencies: pip install -r requirements.txt
3. Run main app: python gradio_full_llm_eval.py
4. Or use standalone tools for specific tasks 